2022-11-02 08:24:40,560 ----------------------------------------------------------------------------------------------------
2022-11-02 08:24:40,561 Model: "TARSTagger(
  (tars_model): SequenceTagger(
    (embeddings): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 128, padding_idx=0)
          (position_embeddings): Embedding(512, 128)
          (token_type_embeddings): Embedding(2, 128)
          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=128, out_features=128, bias=True)
                  (key): Linear(in_features=128, out_features=128, bias=True)
                  (value): Linear(in_features=128, out_features=128, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=128, out_features=128, bias=True)
                  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=128, out_features=512, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=512, out_features=128, bias=True)
                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=128, out_features=128, bias=True)
                  (key): Linear(in_features=128, out_features=128, bias=True)
                  (value): Linear(in_features=128, out_features=128, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=128, out_features=128, bias=True)
                  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=128, out_features=512, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=512, out_features=128, bias=True)
                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=128, out_features=128, bias=True)
          (activation): Tanh()
        )
      )
    )
    (word_dropout): WordDropout(p=0.05)
    (locked_dropout): LockedDropout(p=0.5)
    (linear): Linear(in_features=128, out_features=5, bias=True)
  )
)"
2022-11-02 08:24:40,562 ----------------------------------------------------------------------------------------------------
2022-11-02 08:24:40,562 Corpus: "Corpus: 550 train + 550 dev + 550 test sentences"
2022-11-02 08:24:40,562 ----------------------------------------------------------------------------------------------------
2022-11-02 08:24:40,562 Parameters:
2022-11-02 08:24:40,562  - learning_rate: "0.02"
2022-11-02 08:24:40,563  - mini_batch_size: "8"
2022-11-02 08:24:40,563  - patience: "3"
2022-11-02 08:24:40,563  - anneal_factor: "0.5"
2022-11-02 08:24:40,563  - max_epochs: "10"
2022-11-02 08:24:40,563  - shuffle: "True"
2022-11-02 08:24:40,563  - train_with_dev: "False"
2022-11-02 08:24:40,563  - batch_growth_annealing: "False"
2022-11-02 08:24:40,563 ----------------------------------------------------------------------------------------------------
2022-11-02 08:24:40,563 Model training base path: "train/prajjwal1/bert-tiny"
2022-11-02 08:24:40,563 ----------------------------------------------------------------------------------------------------
2022-11-02 08:24:40,563 Device: cpu
2022-11-02 08:24:40,563 ----------------------------------------------------------------------------------------------------
2022-11-02 08:24:40,563 Embeddings storage mode: cpu
2022-11-02 08:24:40,567 ----------------------------------------------------------------------------------------------------
2022-11-02 08:24:41,863 epoch 1 - iter 6/69 - loss 0.80977111 - samples/sec: 38.91 - lr: 0.020000
2022-11-02 08:24:42,960 epoch 1 - iter 12/69 - loss 0.54294948 - samples/sec: 43.80 - lr: 0.020000
2022-11-02 08:24:44,320 epoch 1 - iter 18/69 - loss 0.44947344 - samples/sec: 35.30 - lr: 0.020000
2022-11-02 08:24:45,457 epoch 1 - iter 24/69 - loss 0.39711453 - samples/sec: 42.25 - lr: 0.020000
2022-11-02 08:24:46,530 epoch 1 - iter 30/69 - loss 0.36660154 - samples/sec: 44.75 - lr: 0.020000
2022-11-02 08:24:47,571 epoch 1 - iter 36/69 - loss 0.34396269 - samples/sec: 46.13 - lr: 0.020000
2022-11-02 08:24:48,607 epoch 1 - iter 42/69 - loss 0.32631448 - samples/sec: 46.36 - lr: 0.020000
2022-11-02 08:24:49,611 epoch 1 - iter 48/69 - loss 0.31129919 - samples/sec: 47.84 - lr: 0.020000
2022-11-02 08:24:50,640 epoch 1 - iter 54/69 - loss 0.30163817 - samples/sec: 46.69 - lr: 0.020000
2022-11-02 08:24:52,142 epoch 1 - iter 60/69 - loss 0.29492543 - samples/sec: 31.97 - lr: 0.020000
2022-11-02 08:24:53,148 epoch 1 - iter 66/69 - loss 0.28722035 - samples/sec: 47.72 - lr: 0.020000
2022-11-02 08:24:53,576 ----------------------------------------------------------------------------------------------------
2022-11-02 08:24:53,576 EPOCH 1 done: loss 0.2844 - lr 0.0200000
2022-11-02 08:25:05,668 DEV : loss 0.09684550387192493 - f1-score (micro avg)  0.1458
2022-11-02 08:25:05,675 BAD EPOCHS (no improvement): 0
2022-11-02 08:25:05,696 saving best model
2022-11-02 08:25:05,717 ----------------------------------------------------------------------------------------------------
2022-11-02 08:25:06,707 epoch 2 - iter 6/69 - loss 0.16571827 - samples/sec: 48.87 - lr: 0.020000
2022-11-02 08:25:07,709 epoch 2 - iter 12/69 - loss 0.17346122 - samples/sec: 47.91 - lr: 0.020000
2022-11-02 08:25:08,802 epoch 2 - iter 18/69 - loss 0.16640520 - samples/sec: 43.98 - lr: 0.020000
2022-11-02 08:25:09,786 epoch 2 - iter 24/69 - loss 0.16332532 - samples/sec: 48.80 - lr: 0.020000
2022-11-02 08:25:10,738 epoch 2 - iter 30/69 - loss 0.16126344 - samples/sec: 50.43 - lr: 0.020000
2022-11-02 08:25:11,789 epoch 2 - iter 36/69 - loss 0.15781414 - samples/sec: 45.71 - lr: 0.020000
2022-11-02 08:25:13,266 epoch 2 - iter 42/69 - loss 0.15450487 - samples/sec: 32.50 - lr: 0.020000
2022-11-02 08:25:14,254 epoch 2 - iter 48/69 - loss 0.15073533 - samples/sec: 48.65 - lr: 0.020000
2022-11-02 08:25:15,369 epoch 2 - iter 54/69 - loss 0.14727056 - samples/sec: 43.06 - lr: 0.020000
2022-11-02 08:25:16,388 epoch 2 - iter 60/69 - loss 0.14408276 - samples/sec: 47.13 - lr: 0.020000
2022-11-02 08:25:17,500 epoch 2 - iter 66/69 - loss 0.14146964 - samples/sec: 43.19 - lr: 0.020000
2022-11-02 08:25:17,993 ----------------------------------------------------------------------------------------------------
2022-11-02 08:25:17,993 EPOCH 2 done: loss 0.1403 - lr 0.0200000
2022-11-02 08:25:30,416 DEV : loss 0.029944043700953916 - f1-score (micro avg)  0.8096
2022-11-02 08:25:30,423 BAD EPOCHS (no improvement): 0
2022-11-02 08:25:30,445 saving best model
2022-11-02 08:25:30,469 ----------------------------------------------------------------------------------------------------
2022-11-02 08:25:31,505 epoch 3 - iter 6/69 - loss 0.11465489 - samples/sec: 46.65 - lr: 0.020000
2022-11-02 08:25:32,603 epoch 3 - iter 12/69 - loss 0.11077885 - samples/sec: 43.75 - lr: 0.020000
2022-11-02 08:25:33,651 epoch 3 - iter 18/69 - loss 0.10521826 - samples/sec: 45.81 - lr: 0.020000
2022-11-02 08:25:34,654 epoch 3 - iter 24/69 - loss 0.10308163 - samples/sec: 47.90 - lr: 0.020000
2022-11-02 08:25:35,764 epoch 3 - iter 30/69 - loss 0.10137146 - samples/sec: 43.27 - lr: 0.020000
2022-11-02 08:25:36,965 epoch 3 - iter 36/69 - loss 0.09816424 - samples/sec: 39.97 - lr: 0.020000
2022-11-02 08:25:38,116 epoch 3 - iter 42/69 - loss 0.09557342 - samples/sec: 41.74 - lr: 0.020000
2022-11-02 08:25:39,384 epoch 3 - iter 48/69 - loss 0.09340503 - samples/sec: 37.88 - lr: 0.020000
2022-11-02 08:25:40,578 epoch 3 - iter 54/69 - loss 0.09279617 - samples/sec: 40.23 - lr: 0.020000
2022-11-02 08:25:41,622 epoch 3 - iter 60/69 - loss 0.09107253 - samples/sec: 46.00 - lr: 0.020000
2022-11-02 08:25:42,646 epoch 3 - iter 66/69 - loss 0.08990798 - samples/sec: 46.91 - lr: 0.020000
2022-11-02 08:25:43,191 ----------------------------------------------------------------------------------------------------
2022-11-02 08:25:43,191 EPOCH 3 done: loss 0.0892 - lr 0.0200000
2022-11-02 08:25:55,974 DEV : loss 0.015639182981947358 - f1-score (micro avg)  0.9276
2022-11-02 08:25:55,981 BAD EPOCHS (no improvement): 0
2022-11-02 08:25:56,006 saving best model
2022-11-02 08:25:56,024 ----------------------------------------------------------------------------------------------------
2022-11-02 08:25:57,101 epoch 4 - iter 6/69 - loss 0.07330348 - samples/sec: 44.84 - lr: 0.020000
2022-11-02 08:25:58,229 epoch 4 - iter 12/69 - loss 0.07519831 - samples/sec: 42.60 - lr: 0.020000
2022-11-02 08:25:59,418 epoch 4 - iter 18/69 - loss 0.06991086 - samples/sec: 40.37 - lr: 0.020000
2022-11-02 08:26:00,560 epoch 4 - iter 24/69 - loss 0.06878424 - samples/sec: 42.08 - lr: 0.020000
2022-11-02 08:26:01,686 epoch 4 - iter 30/69 - loss 0.06807656 - samples/sec: 42.63 - lr: 0.020000
2022-11-02 08:26:03,353 epoch 4 - iter 36/69 - loss 0.06930142 - samples/sec: 28.81 - lr: 0.020000
2022-11-02 08:26:04,444 epoch 4 - iter 42/69 - loss 0.06926198 - samples/sec: 44.02 - lr: 0.020000
2022-11-02 08:26:05,515 epoch 4 - iter 48/69 - loss 0.06897321 - samples/sec: 44.84 - lr: 0.020000
2022-11-02 08:26:06,676 epoch 4 - iter 54/69 - loss 0.06943011 - samples/sec: 41.38 - lr: 0.020000
2022-11-02 08:26:07,839 epoch 4 - iter 60/69 - loss 0.06771358 - samples/sec: 41.31 - lr: 0.020000
2022-11-02 08:26:09,076 epoch 4 - iter 66/69 - loss 0.06615689 - samples/sec: 38.82 - lr: 0.020000
2022-11-02 08:26:09,610 ----------------------------------------------------------------------------------------------------
2022-11-02 08:26:09,612 EPOCH 4 done: loss 0.0662 - lr 0.0200000
2022-11-02 08:26:24,947 DEV : loss 0.009761471398987013 - f1-score (micro avg)  0.9252
2022-11-02 08:26:24,956 BAD EPOCHS (no improvement): 1
2022-11-02 08:26:24,984 ----------------------------------------------------------------------------------------------------
2022-11-02 08:26:26,399 epoch 5 - iter 6/69 - loss 0.05132450 - samples/sec: 34.15 - lr: 0.020000
2022-11-02 08:26:27,841 epoch 5 - iter 12/69 - loss 0.04814423 - samples/sec: 33.30 - lr: 0.020000
2022-11-02 08:26:29,146 epoch 5 - iter 18/69 - loss 0.04897113 - samples/sec: 36.80 - lr: 0.020000
2022-11-02 08:26:30,489 epoch 5 - iter 24/69 - loss 0.05018263 - samples/sec: 35.76 - lr: 0.020000
2022-11-02 08:26:31,769 epoch 5 - iter 30/69 - loss 0.05072468 - samples/sec: 37.52 - lr: 0.020000
2022-11-02 08:26:33,004 epoch 5 - iter 36/69 - loss 0.05056271 - samples/sec: 38.89 - lr: 0.020000
2022-11-02 08:26:34,260 epoch 5 - iter 42/69 - loss 0.05008761 - samples/sec: 38.26 - lr: 0.020000
2022-11-02 08:26:35,721 epoch 5 - iter 48/69 - loss 0.05121085 - samples/sec: 32.87 - lr: 0.020000
2022-11-02 08:26:37,048 epoch 5 - iter 54/69 - loss 0.05138741 - samples/sec: 36.17 - lr: 0.020000
2022-11-02 08:26:38,410 epoch 5 - iter 60/69 - loss 0.05094328 - samples/sec: 35.27 - lr: 0.020000
2022-11-02 08:26:39,883 epoch 5 - iter 66/69 - loss 0.05017126 - samples/sec: 32.60 - lr: 0.020000
2022-11-02 08:26:40,519 ----------------------------------------------------------------------------------------------------
2022-11-02 08:26:40,519 EPOCH 5 done: loss 0.0499 - lr 0.0200000
2022-11-02 08:26:56,203 DEV : loss 0.005727474084001412 - f1-score (micro avg)  0.9758
2022-11-02 08:26:56,212 BAD EPOCHS (no improvement): 0
2022-11-02 08:26:56,240 saving best model
2022-11-02 08:26:56,264 ----------------------------------------------------------------------------------------------------
2022-11-02 08:26:57,775 epoch 6 - iter 6/69 - loss 0.04463767 - samples/sec: 31.96 - lr: 0.020000
2022-11-02 08:26:59,185 epoch 6 - iter 12/69 - loss 0.04788439 - samples/sec: 34.04 - lr: 0.020000
2022-11-02 08:27:01,015 epoch 6 - iter 18/69 - loss 0.04832993 - samples/sec: 26.24 - lr: 0.020000
2022-11-02 08:27:02,357 epoch 6 - iter 24/69 - loss 0.04869726 - samples/sec: 35.79 - lr: 0.020000
2022-11-02 08:27:03,713 epoch 6 - iter 30/69 - loss 0.04724534 - samples/sec: 35.42 - lr: 0.020000
2022-11-02 08:27:05,093 epoch 6 - iter 36/69 - loss 0.04638266 - samples/sec: 34.79 - lr: 0.020000
2022-11-02 08:27:06,593 epoch 6 - iter 42/69 - loss 0.04638932 - samples/sec: 32.02 - lr: 0.020000
2022-11-02 08:27:07,827 epoch 6 - iter 48/69 - loss 0.04534691 - samples/sec: 38.93 - lr: 0.020000
2022-11-02 08:27:09,146 epoch 6 - iter 54/69 - loss 0.04424574 - samples/sec: 36.39 - lr: 0.020000
2022-11-02 08:27:10,357 epoch 6 - iter 60/69 - loss 0.04389907 - samples/sec: 39.68 - lr: 0.020000
2022-11-02 08:27:11,701 epoch 6 - iter 66/69 - loss 0.04420484 - samples/sec: 35.74 - lr: 0.020000
2022-11-02 08:27:12,371 ----------------------------------------------------------------------------------------------------
2022-11-02 08:27:12,371 EPOCH 6 done: loss 0.0436 - lr 0.0200000
2022-11-02 08:27:27,523 DEV : loss 0.004851749825884412 - f1-score (micro avg)  0.9712
2022-11-02 08:27:27,532 BAD EPOCHS (no improvement): 1
2022-11-02 08:27:27,556 ----------------------------------------------------------------------------------------------------
2022-11-02 08:27:28,819 epoch 7 - iter 6/69 - loss 0.03438705 - samples/sec: 38.29 - lr: 0.020000
2022-11-02 08:27:30,138 epoch 7 - iter 12/69 - loss 0.03980259 - samples/sec: 36.41 - lr: 0.020000
2022-11-02 08:27:31,217 epoch 7 - iter 18/69 - loss 0.03883226 - samples/sec: 44.52 - lr: 0.020000
2022-11-02 08:27:32,428 epoch 7 - iter 24/69 - loss 0.03859977 - samples/sec: 39.65 - lr: 0.020000
2022-11-02 08:27:33,719 epoch 7 - iter 30/69 - loss 0.03917940 - samples/sec: 37.22 - lr: 0.020000
2022-11-02 08:27:34,930 epoch 7 - iter 36/69 - loss 0.03771106 - samples/sec: 39.64 - lr: 0.020000
2022-11-02 08:27:36,251 epoch 7 - iter 42/69 - loss 0.03825060 - samples/sec: 36.36 - lr: 0.020000
2022-11-02 08:27:37,423 epoch 7 - iter 48/69 - loss 0.03835325 - samples/sec: 40.97 - lr: 0.020000
2022-11-02 08:27:38,664 epoch 7 - iter 54/69 - loss 0.03775114 - samples/sec: 38.70 - lr: 0.020000
2022-11-02 08:27:39,896 epoch 7 - iter 60/69 - loss 0.03740756 - samples/sec: 38.98 - lr: 0.020000
2022-11-02 08:27:41,171 epoch 7 - iter 66/69 - loss 0.03699218 - samples/sec: 37.67 - lr: 0.020000
2022-11-02 08:27:41,709 ----------------------------------------------------------------------------------------------------
2022-11-02 08:27:41,709 EPOCH 7 done: loss 0.0370 - lr 0.0200000
2022-11-02 08:27:56,180 DEV : loss 0.005015194173597668 - f1-score (micro avg)  0.9747
2022-11-02 08:27:56,189 BAD EPOCHS (no improvement): 2
2022-11-02 08:27:56,217 ----------------------------------------------------------------------------------------------------
2022-11-02 08:27:57,487 epoch 8 - iter 6/69 - loss 0.03527514 - samples/sec: 38.06 - lr: 0.020000
2022-11-02 08:27:59,290 epoch 8 - iter 12/69 - loss 0.03490072 - samples/sec: 26.65 - lr: 0.020000
2022-11-02 08:28:00,626 epoch 8 - iter 18/69 - loss 0.03212201 - samples/sec: 35.92 - lr: 0.020000
2022-11-02 08:28:01,891 epoch 8 - iter 24/69 - loss 0.03160546 - samples/sec: 37.97 - lr: 0.020000
2022-11-02 08:28:03,146 epoch 8 - iter 30/69 - loss 0.03135760 - samples/sec: 38.29 - lr: 0.020000
2022-11-02 08:28:04,407 epoch 8 - iter 36/69 - loss 0.03289387 - samples/sec: 38.08 - lr: 0.020000
2022-11-02 08:28:05,681 epoch 8 - iter 42/69 - loss 0.03264839 - samples/sec: 37.68 - lr: 0.020000
2022-11-02 08:28:06,947 epoch 8 - iter 48/69 - loss 0.03214618 - samples/sec: 37.93 - lr: 0.020000
2022-11-02 08:28:08,411 epoch 8 - iter 54/69 - loss 0.03170894 - samples/sec: 32.80 - lr: 0.020000
2022-11-02 08:28:10,060 epoch 8 - iter 60/69 - loss 0.03193655 - samples/sec: 29.13 - lr: 0.020000
2022-11-02 08:28:11,495 epoch 8 - iter 66/69 - loss 0.03174343 - samples/sec: 33.47 - lr: 0.020000
2022-11-02 08:28:12,247 ----------------------------------------------------------------------------------------------------
2022-11-02 08:28:12,247 EPOCH 8 done: loss 0.0314 - lr 0.0200000
2022-11-02 08:28:29,116 DEV : loss 0.0034204665063119113 - f1-score (micro avg)  0.9842
2022-11-02 08:28:29,125 BAD EPOCHS (no improvement): 0
2022-11-02 08:28:29,153 saving best model
2022-11-02 08:28:29,183 ----------------------------------------------------------------------------------------------------
2022-11-02 08:28:30,514 epoch 9 - iter 6/69 - loss 0.03055378 - samples/sec: 36.30 - lr: 0.020000
2022-11-02 08:28:31,937 epoch 9 - iter 12/69 - loss 0.03162619 - samples/sec: 33.75 - lr: 0.020000
2022-11-02 08:28:33,452 epoch 9 - iter 18/69 - loss 0.02836861 - samples/sec: 31.71 - lr: 0.020000
2022-11-02 08:28:34,901 epoch 9 - iter 24/69 - loss 0.02953016 - samples/sec: 33.14 - lr: 0.020000
2022-11-02 08:28:36,325 epoch 9 - iter 30/69 - loss 0.02950547 - samples/sec: 33.72 - lr: 0.020000
2022-11-02 08:28:37,724 epoch 9 - iter 36/69 - loss 0.02928369 - samples/sec: 34.35 - lr: 0.020000
2022-11-02 08:28:38,959 epoch 9 - iter 42/69 - loss 0.02862679 - samples/sec: 38.88 - lr: 0.020000
2022-11-02 08:28:40,312 epoch 9 - iter 48/69 - loss 0.02920572 - samples/sec: 35.50 - lr: 0.020000
2022-11-02 08:28:41,754 epoch 9 - iter 54/69 - loss 0.02844434 - samples/sec: 33.30 - lr: 0.020000
2022-11-02 08:28:43,119 epoch 9 - iter 60/69 - loss 0.02858834 - samples/sec: 35.18 - lr: 0.020000
2022-11-02 08:28:44,435 epoch 9 - iter 66/69 - loss 0.02859035 - samples/sec: 36.51 - lr: 0.020000
2022-11-02 08:28:45,123 ----------------------------------------------------------------------------------------------------
2022-11-02 08:28:45,123 EPOCH 9 done: loss 0.0286 - lr 0.0200000
2022-11-02 08:29:01,117 DEV : loss 0.006152148921022289 - f1-score (micro avg)  0.9819
2022-11-02 08:29:01,126 BAD EPOCHS (no improvement): 1
2022-11-02 08:29:01,151 ----------------------------------------------------------------------------------------------------
2022-11-02 08:29:02,512 epoch 10 - iter 6/69 - loss 0.02382199 - samples/sec: 35.48 - lr: 0.020000
2022-11-02 08:29:04,288 epoch 10 - iter 12/69 - loss 0.02592261 - samples/sec: 27.04 - lr: 0.020000
2022-11-02 08:29:05,559 epoch 10 - iter 18/69 - loss 0.02614993 - samples/sec: 37.77 - lr: 0.020000
2022-11-02 08:29:06,730 epoch 10 - iter 24/69 - loss 0.02659399 - samples/sec: 41.02 - lr: 0.020000
2022-11-02 08:29:08,071 epoch 10 - iter 30/69 - loss 0.02650152 - samples/sec: 35.83 - lr: 0.020000
2022-11-02 08:29:09,375 epoch 10 - iter 36/69 - loss 0.02613262 - samples/sec: 36.83 - lr: 0.020000
2022-11-02 08:29:10,703 epoch 10 - iter 42/69 - loss 0.02634329 - samples/sec: 36.16 - lr: 0.020000
2022-11-02 08:29:11,975 epoch 10 - iter 48/69 - loss 0.02642345 - samples/sec: 37.74 - lr: 0.020000
2022-11-02 08:29:13,119 epoch 10 - iter 54/69 - loss 0.02579518 - samples/sec: 41.98 - lr: 0.020000
2022-11-02 08:29:14,361 epoch 10 - iter 60/69 - loss 0.02575024 - samples/sec: 38.68 - lr: 0.020000
2022-11-02 08:29:15,824 epoch 10 - iter 66/69 - loss 0.02613826 - samples/sec: 32.83 - lr: 0.020000
2022-11-02 08:29:16,389 ----------------------------------------------------------------------------------------------------
2022-11-02 08:29:16,389 EPOCH 10 done: loss 0.0261 - lr 0.0200000
2022-11-02 08:29:31,196 DEV : loss 0.002728493421385945 - f1-score (micro avg)  0.9867
2022-11-02 08:29:31,204 BAD EPOCHS (no improvement): 0
2022-11-02 08:29:31,228 saving best model
2022-11-02 08:29:31,273 ----------------------------------------------------------------------------------------------------
2022-11-02 08:29:31,274 loading file train/prajjwal1/bert-tiny/best-model.pt
2022-11-02 08:29:38,484 No model_max_length in Tokenizer's config.json - setting it to 512. Specify desired model_max_length by passing it as attribute to embedding instance.
2022-11-02 08:29:52,091 0.9873	0.9861	0.9867	0.9737
2022-11-02 08:29:52,091 
Results:
- F-score (micro) 0.9867
- F-score (macro) 0.9863
- Accuracy 0.9737

By class:
               precision    recall  f1-score   support

         time     0.9968    0.9984    0.9976       622
haircut_style     0.9873    0.9596    0.9733       569
    dayofweek     0.9764    1.0000    0.9881       538

    micro avg     0.9873    0.9861    0.9867      1729
    macro avg     0.9868    0.9860    0.9863      1729
 weighted avg     0.9873    0.9861    0.9866      1729
  samples avg     0.9737    0.9737    0.9737      1729

2022-11-02 08:29:52,092 ----------------------------------------------------------------------------------------------------
